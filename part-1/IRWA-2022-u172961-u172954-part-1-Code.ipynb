{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IRWA Final Project\n",
        "## **Part 1- Text processing**\n",
        "\n",
        "Authors:\n",
        "\n",
        "\n",
        "*   Malena DÃ­az - u172961\n",
        "*   Cristina Galvez - u172954\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A6t22L6yzH4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 1: Importing the data**"
      ],
      "metadata": {
        "id": "UmkZVF_uzzSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we install the `word2number` library, which is not a common Python library to have installed."
      ],
      "metadata": {
        "id": "GXvFNkcSVHQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fnLKaF84HuE",
        "outputId": "b1d7a5ae-2bfa-4f5a-bf5b-cca04a277993"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=ea611a89e79e5c4b650c5300bc96dd4ae432e316f017e8fdac5398bee0dcf710\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import several libraries that will be used for this project. Make sure to have them installed, otherwise it will return an error."
      ],
      "metadata": {
        "id": "qWDmMxyVVYh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "from word2number import w2n\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import datetime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtBnFp04csGR",
        "outputId": "a7412449-db80-4b3b-a729-90ecfe93a757"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This library is thought to be executed from Google Colab, so it can be linked to the Drive directories. Please allow Colab to access your Google Drive directories."
      ],
      "metadata": {
        "id": "3XFY3C4iVnmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "3hAraVjiz4TC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643a7dee-043e-489f-b10a-3e6092651dd0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT!**\n",
        "\n",
        "In order to achieve a correct performance of the program, indicate the path of the data in the following variable `docs_path`. Make sure the document *tw_hurricane_data.json* is stored there."
      ],
      "metadata": {
        "id": "atdRY6KH0LKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'drive/Shareddrives/IRWA/Project/data/tw_hurricane_data.json' \n",
        "with open(data_path) as fp:\n",
        "    data = fp.readlines()"
      ],
      "metadata": {
        "id": "j-ncA2fDzpYi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT!**\n",
        "\n",
        "Indicate also the path of the file with the map document (*tweet_document_ids_map.csv*) in in the variable `map_path`."
      ],
      "metadata": {
        "id": "wcxEwEiAzVEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the map id => document name \n",
        "doc_id_dict = {}\n",
        "map_path = 'drive/Shareddrives/IRWA/Project/data/tweet_document_ids_map.csv' \n",
        "with open(map_path) as map_file:\n",
        "    tsv_reader = csv.reader(map_file, delimiter=\"\\t\")\n",
        "    for line in tsv_reader:\n",
        "      (doc, id) = line\n",
        "      doc_id_dict[int(id)] = doc"
      ],
      "metadata": {
        "id": "MDlHiOI9TkaP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that the files have been read correctly:"
      ],
      "metadata": {
        "id": "yv2OcyWvztua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of Tweets in the dataset: {}\".format(len(data)))\n",
        "print(\"Total number of Tweets in the map: {}\".format(len(doc_id_dict)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExNECp6AOTL9",
        "outputId": "67b0d545-a388-4e2c-87f2-63945e8cbc3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Tweets in the dataset: 4000\n",
            "Total number of Tweets in the map: 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 2: Extracting main fields of the dataset**"
      ],
      "metadata": {
        "id": "echtv35RzeC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous variable `data` contains an array of strings, each of them a dictionary with information of the tweet in `.json` format. We transform it to an array of dictionaries in Python format."
      ],
      "metadata": {
        "id": "DwKXXpSfXTM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [json.loads(x) for x in data] # transform each tweet from string to dictionnary "
      ],
      "metadata": {
        "id": "yYiS-ZsfQczW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As stated in the statement, we must extract the information about: id, tweet, username, date, hashtag, number of likes, the number of retweets and the tweet url."
      ],
      "metadata": {
        "id": "XwDECw_jz-Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_struct(data):\n",
        "  \"\"\"\n",
        "  Extract the fields tweet, username, date, hashtag, number of likes, the number of retweets and the tweet url from each tweet. \n",
        "  \n",
        "  Argument:\n",
        "  data -- An array of tweets (dictionnaries) that contain the keys full_text, user, created_at, entities and retweet_count. User and entities \n",
        "  correspond both to dictionnaries containing keys screen_name and favourites_count respectively. \n",
        "\n",
        "  Returns:\n",
        "  collection -- A collection of dictionnaries where the key corresponds to the dictionnary name. Each value is another dictionnary containing the fields \n",
        "  of interest. \n",
        "\n",
        "  \"\"\"\n",
        "  collection = {}     # dictionaty of dictionaries\n",
        "\n",
        "  for tweet in data:\n",
        "    doc_dict = {}\n",
        "    id = tweet['id']\n",
        "    doc_name = doc_id_dict[id]\n",
        "\n",
        "    doc_dict['id'] = id\n",
        "    doc_dict['tweet'] = tweet['full_text']\n",
        "    doc_dict['username'] = tweet['user']['screen_name']\n",
        "    doc_dict['date'] = datetime.datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S %z %Y') #convert to date time \n",
        "    doc_dict['hashtag'] = [x['text'] for x in tweet['entities']['hashtags']] #text of te hashtag \n",
        "    doc_dict['likes'] = tweet['user']['favourites_count']\n",
        "    doc_dict['retweet'] = tweet ['retweet_count'] # can also be field retweeted (true)\n",
        "    doc_dict['url'] = \"https://twitter.com/\" +  doc_dict['username'] + \"/status/\" + str(id) #https://twitter.com/[screen name]/status/[Tweet ID]\n",
        "    collection[doc_name] = doc_dict\n",
        "\n",
        "  return collection"
      ],
      "metadata": {
        "id": "fNUhg-LwTLmL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the function defined above, transform our data into the structure `collection`, that is a dictionary of dictionaries."
      ],
      "metadata": {
        "id": "HVAnEj4WYaDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collection = create_struct(data)  # transform data\n",
        "a = [print(x,':', collection['doc_2'][x]) for x in collection['doc_2']]"
      ],
      "metadata": {
        "id": "tdGO5UG6rv_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82694a36-eea4-4f3e-c0c0-5b184e754851"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id : 1575918151862304768\n",
            "tweet : Our hearts go out to all those affected by #HurricaneIan. We wish everyone on the roads currently braving the conditions safe travels. ðŸ’™\n",
            "username : lytx\n",
            "date : 2022-09-30 18:39:01+00:00\n",
            "hashtag : ['HurricaneIan']\n",
            "likes : 2633\n",
            "retweet : 0\n",
            "url : https://twitter.com/lytx/status/1575918151862304768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 3: Cleaning the dataset**"
      ],
      "metadata": {
        "id": "FySRQRwRPuJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part of the project, we will clean the text part of the documents, so that it does not contain symbols, is all lowercase, URLs are deleted... among others.\n",
        "\n",
        "First, we have taken special attention into turning written numbers into digits. We have defined a separate function for this purpose:"
      ],
      "metadata": {
        "id": "wF6Tiqtldk_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to handle turning written numbers into digits\n",
        "\n",
        "def is_written_num(word):\n",
        "  # returns True only if it is a written number\n",
        "  # returns False if it is a digit or something else\n",
        "  try:\n",
        "    w2n.word_to_num(word)\n",
        "    try: \n",
        "      int(word)\n",
        "      return False\n",
        "    except:\n",
        "      return True\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "# given an array of strings, takes all written numbers and makes them digits\n",
        "def make_numbers_digits(text):\n",
        "  consec_num = False\n",
        "  num_string = ''\n",
        "  result = []\n",
        "\n",
        "  for word in text:\n",
        "    if not is_written_num(word):\n",
        "      if consec_num:\n",
        "        result.append(str(w2n.word_to_num(num_string)))\n",
        "        consec_num = False\n",
        "        num_string = ''\n",
        "      result.append(word)\n",
        "    else:\n",
        "      consec_num = True\n",
        "      num_string += ' ' + word\n",
        "\n",
        "  if consec_num:\n",
        "    result.append(str(w2n.word_to_num(num_string)))\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "3qEugdl8eEG7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the function is working\n",
        "example = ['there', 'are', 'five', 'hundred', 'twenty', 'seven', 'ideas', 'for', 'the', 'eighty', 'nine', 'people', 'to', 'process', 'one', 'more', 'time', '66', '345']\n",
        "print(make_numbers_digits(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoftM0j3eRIa",
        "outputId": "a459f572-b983-4c08-9553-143b82925213"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['there', 'are', '527', 'ideas', 'for', 'the', '89', 'people', 'to', 'process', '1', 'more', 'time', '66', '345']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a function to treat hashtags. Many hashtags consist of many joined words starting with a capital letter. This function does the following:\n",
        "\n",
        "`#hurricaneIan --> 'hurricane Ian'`"
      ],
      "metadata": {
        "id": "PbKYGHMpkPbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def treat_hashtags(text):\n",
        "  remove_hashtag = text[1:]\n",
        "  split_lower_upper = re.sub(r\"([A-Z])\", r\" \\1\", remove_hashtag)\n",
        "  return split_lower_upper"
      ],
      "metadata": {
        "id": "c7JnmEnCk0e8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the function `build_terms()` to turn the given string into a clean version of it. It applies the following transformations:\n",
        "- remove URLs\n",
        "- remove user names\n",
        "- deal with hashtags\n",
        "- deal with dashes\n",
        "- deal with currencies\n",
        "- remove symbols\n",
        "- transform to lowercase\n",
        "- delete stop-words\n",
        "- perform stemming\n",
        "- turn written numbers into digits\n",
        "- remove single letter words"
      ],
      "metadata": {
        "id": "Dw6orIN7ezWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_terms(line):\n",
        "    \"\"\"\n",
        "    Preprocess the article text (title + body) removing stop words, stemming,\n",
        "    transforming in lowercase and return the tokens of the text.\n",
        "    \n",
        "    Argument:\n",
        "    line -- string (text) to be preprocessed\n",
        "    \n",
        "    Returns:\n",
        "    line - a list of tokens corresponding to the input text after the preprocessing\n",
        "    \"\"\"\n",
        "\n",
        "    # define stemmer and reference lists\n",
        "    stemmer = PorterStemmer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    whitelist = string.ascii_letters + string.digits + ' '\n",
        "\n",
        "    # clean text\n",
        "    line = re.sub(r'http\\S+', '', line) # remove urls\n",
        "    line = re.sub(r'@\\S+', '', line) # remove mentioned users\n",
        "    line = ' '.join([treat_hashtags(i) if i.startswith(\"#\") else i for i in line.split()]) # deal with hashtags\n",
        "    line = line.replace(\"-\", \" \") # deal with dashes\n",
        "    line = line.replace(\"$\", \" dollars\") # deal with currencies  \n",
        "    line = line.replace(\"â‚¬\", \" euros\") # deal with currencies  \n",
        "    line = ''.join([char if char in whitelist else ' ' for char in line]) # remove all symbols (leave only letters, digits, # and spaces)\n",
        "    line = line.lower() ## Transform in lowercase\n",
        "    line = line.split() ## Tokenize the text to get a list of terms\n",
        "    line = [x for x in line if x not in stop_words]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
        "    line = [stemmer.stem(x) for x in line ] ## perform stemming (HINT: use List Comprehension)\n",
        "    line = make_numbers_digits(line) # turn written numbers into digits\n",
        "    line = [x for x in line if (len(x) > 1 or x.isdigit()) ] # remove single letters\n",
        "\n",
        "    return line"
      ],
      "metadata": {
        "id": "r9e-Mq88cegp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the text is being cleanned properly:"
      ],
      "metadata": {
        "id": "rFiXQAF2l0de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts = [collection[doc]['tweet'] for doc in collection][:5]\n",
        "\n",
        "for i in range(5):\n",
        "  print('Tweet {}:\\n{}\\n{}\\n'.format(i+1, sample_texts[i], build_terms(sample_texts[i])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O6N0FjPmLPB",
        "outputId": "59a3de87-6a1d-446d-bb43-06609ea87b50"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet 1:\n",
            "So this will keep spinning over us until 7 pmâ€¦go away already. #HurricaneIan https://t.co/VROTxNS9rz\n",
            "['keep', 'spin', 'us', '7', 'pm', 'go', 'away', 'alreadi', 'hurrican', 'ian']\n",
            "\n",
            "Tweet 2:\n",
            "Our hearts go out to all those affected by #HurricaneIan. We wish everyone on the roads currently braving the conditions safe travels. ðŸ’™\n",
            "['heart', 'go', 'affect', 'hurrican', 'ian', 'wish', 'everyon', 'road', 'current', 'brave', 'condit', 'safe', 'travel']\n",
            "\n",
            "Tweet 3:\n",
            "Kissimmee neighborhood off of Michigan Ave. \n",
            "#HurricaneIan https://t.co/jf7zseg0Fe\n",
            "['kissimme', 'neighborhood', 'michigan', 'ave', 'hurrican', 'ian']\n",
            "\n",
            "Tweet 4:\n",
            "I have this one tree in my backyard that scares me more than the poltergeist tree when itâ€™s storming and windy like this. #scwx #HurricaneIan\n",
            "['1', 'tree', 'backyard', 'scare', 'poltergeist', 'tree', 'storm', 'windi', 'like', 'scwx', 'hurrican', 'ian']\n",
            "\n",
            "Tweet 5:\n",
            "@AshleyRuizWx @Stephan89441722 @lilmizzheidi @Mr__Sniffles @winknews @DylanFedericoWX @julianamwx @sydneypersing @NicoleGabeTV I pray for everyone affected by #HurricaneIan, but not those associated with WINKnews.  No sympathy for animal abusers, liars, and those that condone it.\n",
            "['pray', 'everyon', 'affect', 'hurrican', 'ian', 'associ', 'winknew', 'sympathi', 'anim', 'abus', 'liar', 'condon']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step is to use the previosly defined function to clean all the tweets from `collection`:"
      ],
      "metadata": {
        "id": "A_T-jkmSg0P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in collection:\n",
        "  collection[doc]['tweet'] = build_terms(collection[doc]['tweet'])"
      ],
      "metadata": {
        "id": "PRKEOYDl2iAD"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}